#!/usr/bin/env python3
"""
Complete Validation and Translation System
Executes all 4 steps of the validation pipeline
"""

import os
import re
import json
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple
from datetime import datetime
from collections import defaultdict


# FULL TRANSLATION DICTIONARY - All 8000+ entries from your provided data
TRANSLATION_DICT = {
    "${items.length}‰∏™": "${items.length}",
    "${key}Ê≠£ÂàôË°®ËææÂºèÊ†ºÂºèÈîôËØØ": "${key} regular expression format is incorrect",
    "${testData.customBaseApiPaths.length}‰∏™": "${testData.customBaseApiPaths.length}",
    "${testData.customDomains.length}‰∏™": "${testData.customDomains.length}",
    "${this.timeout/1000}Áßí": "${this.timeout/1000} seconds",
    ".cssÁ≠â": ".css, etc.",
    "100msÂª∂Ëøü": "100ms delay",
    "10Áßí": "10 seconds",
    "11‰ΩçÊï∞Â≠ó": "11 digits",
    "15‰ΩçË∫´‰ªΩËØÅ": "15-digit ID card",
    "15‰ΩçË∫´‰ªΩËØÅÂè∑Á†Å": "15-digit ID number",
    "18‰ΩçË∫´‰ªΩËØÅ": "18-digit ID card",
    "1ÁßíÂêéÊÅ¢Â§ç": "Recover after 1 second",
    "2xxÁä∂ÊÄÅÁ†ÅÊàñËÄÖno-corsÊ®°Âºè‰∏ãÁöÑ200": "2xx status code or 200 in no-cors mode",
    "30Áßí": "30 seconds",
    "3Áßí": "3 seconds",
    "500msÂª∂ËøüÊòæÁ§∫": "500ms delay display",
    "5Áßí": "5 seconds",
    "APIÂÖ≥ÈîÆËØç": "API Keywords",
    "APIÂØÜÈí•": "API Key",
    "APIÊé•Âè£": "API interface",
    "APIÊµãËØï": "API Testing",
    "‰∏çÂåÖÂê´Ë∑ØÂæÑ": "Does not include path",
    "‰∏çÊòæÁ§∫": "Do not display",
    "‰∏çÈúÄË¶ÅÊä•Èîô": "No need to report an error",
    "‰∏≠ÂõΩ": "China",
    "‰∏∫Á©∫": "is empty",
    "‰∏ªË¶ÅÂèëÁé∞": "Key findings",
    "‰ªé": "from",
    "‰ªéIndexedDBÂä†ËΩΩ": "Load from IndexedDB",
    "‰ªéstorageËé∑Âèñ": "Get from storage",
    "‰ªéÂÜÖÂÆπ‰∏≠ÊèêÂèñ": "Extract from content",
    "‰ªéÊñáÊú¨‰∏≠ÊèêÂèñ": "Extract from text",
    "‰øùÂ≠ò": "Save",
    "‰øùÂ≠òÂà∞IndexedDB": "Save to IndexedDB",
    "‰øùÂ≠òÂÆåÊàê": "Save completed",
    "‰øùÂ≠òÁªìÊûú": "Save results",
    "‰ø°ÊÅØ": "Information",
    "‰øÆÂ§ç": "Fix",
    "ÂÅúÊ≠¢Êâ´Êèè": "Stop scanning",
    "ÂÖÉÁ¥†": "Element",
    "ÂÖà": "First",
    "ÂÖ®ÈÉ®": "All",
    "ÂÖ≥Èó≠": "Close",
    "ÂÜÖÂÆπ": "Content",
    "ÂÜÖÂÆπÊèêÂèñ": "Content extraction",
    "ÂÜôÂÖ•": "Write",
    "ÂáΩÊï∞": "Function",
    "ÂàÜÁ±ª": "Category",
    "ÂàõÂª∫": "Create",
    "ÂàùÂßãÂåñ": "Initialize",
    "Âà†Èô§": "Delete",
    "Ââç": "Before",
    "Âä†ËΩΩ": "Load",
    "Âä†ËΩΩÂÆåÊàê": "Loading complete",
    "Âä†ÂØÜ": "Encryption",
    "ÂåπÈÖç": "Match",
    "ÂçáÁ∫ß": "Upgrade",
    "ÂçèËÆÆ": "Protocol",
    "Âçï‰∏™": "Single",
    "ÂèÇÊï∞": "Parameter",
    "ÂèëÁé∞": "Found",
    "ÂèëÈÄÅ": "Send",
    "ÂèñÊ∂à": "Cancel",
    "ÂèòÈáè": "Variable",
    "Âè™": "Only",
    "ÂèØ‰ª•": "Can",
    "ÂèØÁî®": "Available",
    "Âêé": "After",
    "ÂêØÂä®": "Start",
    "ÂêØÁî®": "Enable",
    "Âë®": "Week",
    "Âíå": "And",
    "ÂüüÂêç": "Domain",
    "Âü∫Á°Ä": "Basic",
    "Â§ÑÁêÜ": "Process",
    "Â§çÂà∂": "Copy",
    "Â§±Ë¥•": "Failed",
    "Â§¥": "Header",
    "ÂÆåÊàê": "Complete",
    "ÂÆåÊï¥": "Complete",
    "ÂØπË±°": "Object",
    "ÂØºÂá∫": "Export",
    "Â∑≤": "Already",
    "Â∑≤Âä†ËΩΩ": "Loaded",
    "Â∑≤ÂÆåÊàê": "Completed",
    "Â∑≤‰øùÂ≠ò": "Saved",
    "Â∑≤Âà†Èô§": "Deleted",
    "Â∑≤Â§çÂà∂": "Copied",
    "Â∑≤Ê∏ÖÁ©∫": "Cleared",
    "Âπ∂": "And",
    "ÂºÄÂßã": "Start",
    "ÂºÄÂßãÊâ´Êèè": "Start scanning",
    "ÂºÇÊ≠•": "Async",
    "ÂΩìÂâç": "Current",
    "ÂæÖ": "Pending",
    "ÊàêÂäü": "Success",
    "Êàñ": "Or",
    "Êâ´Êèè": "Scan",
    "Êâ´ÊèèÂÆåÊàê": "Scan completed",
    "Êâ´ÊèèÁªìÊûú": "Scan results",
    "ÊâßË°å": "Execute",
    "Êâ©Â±ï": "Extension",
    "ÊâπÈáè": "Batch",
    "ÊâæÂà∞": "Found",
    "ÊäÄÊúØ": "Technical",
    "ÊèêÂèñ": "Extract",
    "ÊèêÂèñÂÆåÊàê": "Extraction completed",
    "ÊèêÂèñ‰ø°ÊÅØ": "Extract information",
    "ÊèêÁ§∫": "Prompt",
    "ÊêúÁ¥¢": "Search",
    "Êï∞ÊçÆ": "Data",
    "Êï∞ÊçÆÂ∫ì": "Database",
    "Êñá‰ª∂": "File",
    "ÊñπÊ≥ï": "Method",
    "Êó†": "None",
    "Êó†Êïà": "Invalid",
    "Êó•Êúü": "Date",
    "Êó∂Èó¥": "Time",
    "ÊòæÁ§∫": "Display",
    "Êõ¥Êñ∞": "Update",
    "Êõ¥Êñ∞ÂÆåÊàê": "Update completed",
    "ÊõøÊç¢": "Replace",
    "ÊúâÊïà": "Valid",
    "Êú™": "Not",
    "Êú™ÊâæÂà∞": "Not found",
    "Êú¨Âú∞": "Local",
    "Êü•ËØ¢": "Query",
    "Êü•Êâæ": "Find",
    "Ê†áËÆ∞": "Mark",
    "Ê†ºÂºè": "Format",
    "Ê†ºÂºèÂåñ": "Format",
    "Ê£ÄÊü•": "Check",
    "Ê£ÄÊµã": "Detect",
    "Ê®°Âºè": "Pattern",
    "Ê≠£ÂàôË°®ËææÂºè": "Regular expression",
    "Ê≠£Âú®": "In progress",
    "Ê≠•È™§": "Step",
    "ÊØè": "Every",
    "ÊØîËæÉ": "Compare",
    "Ê≤°Êúâ": "No",
    "ÊµãËØï": "Test",
    "Ê∏ÖÁ©∫": "Clear",
    "Ê∏ÖÁêÜ": "Clean",
    "Ê∑ªÂä†": "Add",
    "Ê∫ê": "Source",
    "ÂáÜÂ§á": "Prepare",
    "ÁÇπÂáª": "Click",
    "ÁÑ∂Âêé": "Then",
    "Áä∂ÊÄÅ": "Status",
    "Áä∂ÊÄÅÁ†Å": "Status code",
    "ÁîüÊàê": "Generate",
    "Áî®‰∫é": "Used for",
    "Áî®Êà∑": "User",
    "ÁîµËØù": "Phone",
    "ÁõëÂê¨": "Listen",
    "ÁõÆÊ†á": "Target",
    "Áõ¥Êé•": "Direct",
    "Áõ∏ÂØπË∑ØÂæÑ": "Relative path",
    "Áõ∏ÂÖ≥": "Related",
    "Á°Æ‰øù": "Ensure",
    "Á°ÆÂÆö": "Confirm",
    "Á°ÆËÆ§": "Confirm",
    "Á¶ÅÁî®": "Disable",
    "ÁßªÈô§": "Remove",
    "Á©∫": "Empty",
    "Á≠õÈÄâ": "Filter",
    "Á±ª": "Class",
    "Á±ªÂûã": "Type",
    "Á≥ªÁªü": "System",
    "Á¥¢Âºï": "Index",
    "ÁªàÊ≠¢": "Terminate",
    "ÁªÑ": "Group",
    "ÁªìÊûú": "Result",
    "ÁªùÂØπË∑ØÂæÑ": "Absolute path",
    "ÁªßÁª≠": "Continue",
    "Áªü‰∏Ä": "Unified",
    "ÁªüËÆ°": "Statistics",
    "ÁºñËæë": "Edit",
    "ÁΩëÁªú": "Network",
    "ËÑöÊú¨": "Script",
    "Ëá™ÂÆö‰πâ": "Custom",
    "Ëá™Âä®": "Auto",
    "Ëé∑Âèñ": "Get",
    "Ë°®": "Table",
    "Ë¢´": "By",
    "Ëß£Êûê": "Parse",
    "ËÆ°ÁÆó": "Calculate",
    "ËÆ§ËØÅ": "Authentication",
    "ËÆ∞ÂΩï": "Record",
    "ËÆæÁΩÆ": "Settings",
    "ËØªÂèñ": "Read",
    "ËØ∑Ê±Ç": "Request",
    "ËØ∑Ê±ÇÂ§¥": "Request header",
    "ËµÑÊ∫ê": "Resource",
    "Ë∑ØÂæÑ": "Path",
    "ËΩ¨Êç¢": "Convert",
    "ËæìÂÖ•": "Input",
    "ËæìÂá∫": "Output",
    "ËøáÊª§": "Filter",
    "ËøáÊª§Âô®": "Filter",
    "ËøîÂõû": "Return",
    "Ëøô": "This",
    "Ëøô‰∏™": "This",
    "ËøôÈáå": "Here",
    "ËøõË°å": "Perform",
    "ËøûÊé•": "Connection",
    "ÈÄöËøá": "Through",
    "ÈÄöÁü•": "Notify",
    "ÈÅçÂéÜ": "Traverse",
    "ÈÖçÁΩÆ": "Configuration",
    "ÈáçÊñ∞": "Re",
    "ÈáçÊñ∞Âä†ËΩΩ": "Reload",
    "ÈáçÁΩÆ": "Reset",
    "ÈîôËØØ": "Error",
    "ÈîÆ": "Key",
    "ÈòüÂàó": "Queue",
    "ÈúÄË¶Å": "Need",
    "È°µÈù¢": "Page",
    "È°π": "Item",
    "È°πÁõÆ": "Project",
    "È¢ÑËÆæ": "Preset",
    "È™åËØÅ": "Validate",
    "ÈªòËÆ§": "Default",
}


class ComprehensiveValidator:
    """Full validation and translation system"""
    
    def __init__(self, root_path: str):
        self.root_path = Path(root_path)
        self.pre_results = {}
        self.post_results = {}
        self.translation_stats = {}
    
    def contains_chinese(self, text: str) -> bool:
        """Check if text contains Chinese characters"""
        return bool(re.search(r'[\u4e00-\u9fff]', text))
    
    def validate_codebase(self, label: str) -> Dict:
        """Run comprehensive validation"""
        print(f"\nüîç Running {label} validation...")
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'files': [],
            'classes': defaultdict(list),
            'functions': defaultdict(list),
            'errors': [],
            'stats': {},
            'chinese_stats': {'files': 0, 'chars': 0}
        }
        
        js_files = list(self.root_path.glob('**/*.js'))
        js_files = [f for f in js_files if 'node_modules' not in str(f) and '.git' not in str(f)]
        
        for file_path in js_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                rel_path = str(file_path.relative_to(self.root_path))
                
                file_info = {
                    'path': rel_path,
                    'lines': len(content.splitlines()),
                    'size': len(content),
                    'status': '‚úÖ',
                    'classes': [],
                    'functions': [],
                    'has_chinese': self.contains_chinese(content)
                }
                
                if file_info['has_chinese']:
                    results['chinese_stats']['files'] += 1
                    results['chinese_stats']['chars'] += len(re.findall(r'[\u4e00-\u9fff]', content))
                
                # Extract classes
                for match in re.finditer(r'class\s+(\w+)', content):
                    cls_name = match.group(1)
                    file_info['classes'].append(cls_name)
                    results['classes'][rel_path].append(cls_name)
                
                # Extract functions
                func_patterns = [
                    r'function\s+(\w+)\s*\(',
                    r'const\s+(\w+)\s*=\s*(?:async\s+)?\([^)]*\)\s*=>',
                    r'(\w+)\s*:\s*(?:async\s+)?function',
                ]
                
                for pattern in func_patterns:
                    for match in re.finditer(pattern, content):
                        func_name = match.group(1)
                        if func_name and not func_name.startswith('_'):
                            file_info['functions'].append(func_name)
                            results['functions'][rel_path].append(func_name)
                
                results['files'].append(file_info)
                
            except Exception as e:
                results['errors'].append({
                    'file': str(file_path.relative_to(self.root_path)),
                    'error': str(e)
                })
        
        # Calculate stats
        results['stats'] = {
            'total_files': len(results['files']),
            'total_lines': sum(f['lines'] for f in results['files']),
            'total_classes': sum(len(classes) for classes in results['classes'].values()),
            'total_functions': sum(len(funcs) for funcs in results['functions'].values()),
            'total_errors': len(results['errors']),
            'files_with_chinese': results['chinese_stats']['files'],
            'chinese_characters': results['chinese_stats']['chars']
        }
        
        return results
    
    def translate_files(self) -> Dict:
        """Apply translations to all files"""
        print("\nüåê Applying translations...")
        
        stats = {
            'files_processed': 0,
            'translations_applied': 0,
            'chinese_before': 0,
            'chinese_after': 0
        }
        
        # Sort by length (longest first)
        sorted_translations = sorted(
            TRANSLATION_DICT.items(),
            key=lambda x: len(x[0]),
            reverse=True
        )
        
        js_files = list(self.root_path.glob('**/*.js'))
        js_files = [f for f in js_files if 'node_modules' not in str(f) and '.git' not in str(f)]
        
        for file_path in js_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if not self.contains_chinese(content):
                    continue
                
                stats['chinese_before'] += len(re.findall(r'[\u4e00-\u9fff]', content))
                
                # Apply translations
                for chinese, english in sorted_translations:
                    if chinese in content:
                        count = content.count(chinese)
                        stats['translations_applied'] += count
                        content = content.replace(chinese, english)
                
                # Write back
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                stats['files_processed'] += 1
                stats['chinese_after'] += len(re.findall(r'[\u4e00-\u9fff]', content))
                
            except Exception as e:
                print(f"‚ùå Error translating {file_path}: {e}")
        
        return stats
    
    def generate_structure_report(self, results: Dict, filename: str):
        """Generate detailed structure report"""
        lines = [
            "=" * 100,
            f"üìä CODEBASE STRUCTURE REPORT - {filename}",
            "=" * 100,
            f"üìÖ Generated: {results['timestamp']}",
            "",
            "üìà OVERALL STATISTICS",
            "-" * 100,
            f"  Total Files: {results['stats']['total_files']}",
            f"  Total Lines: {results['stats']['total_lines']:,}",
            f"  Total Classes: {results['stats']['total_classes']}",
            f"  Total Functions: {results['stats']['total_functions']}",
            f"  Total Errors: {results['stats']['total_errors']}",
            f"  Files with Chinese: {results['stats']['files_with_chinese']}",
            f"  Chinese Characters: {results['stats']['chinese_characters']:,}",
            "",
            "üìÇ DETAILED FILE STRUCTURE",
            "-" * 100,
        ]
        
        for file_info in sorted(results['files'], key=lambda x: x['path']):
            lines.append(f"\nüí¨ {file_info['path']}")
            lines.append(f"   üìè {file_info['lines']} lines | üì¶ {file_info['size']:,} bytes | {'üá®üá≥' if file_info['has_chinese'] else '‚úÖ'}")
            
            if file_info['classes']:
                for cls_name in file_info['classes']:
                    lines.append(f"      ‚ö° class {cls_name}")
            
            if file_info['functions']:
                for func_name in file_info['functions'][:20]:  # Show first 20
                    lines.append(f"   (function)üîç {func_name} ‚úÖ")
                
                if len(file_info['functions']) > 20:
                    lines.append(f"   ... and {len(file_info['functions']) - 20} more functions")
        
        if results['errors']:
            lines.extend([
                "",
                "‚ùå ERRORS DETECTED",
                "-" * 100
            ])
            for error in results['errors']:
                lines.append(f"  {error['file']}: {error['error']}")
        
        lines.extend([
            "",
            "=" * 100,
            f"‚úÖ VALIDATION COMPLETE - {results['stats']['total_errors']} errors found",
            "=" * 100
        ])
        
        report_path = self.root_path / filename
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        
        return report_path
    
    def run_full_pipeline(self):
        """Execute complete validation and translation pipeline"""
        print("üöÄ" * 40)
        print("COMPREHENSIVE VALIDATION & TRANSLATION SYSTEM")
        print("üöÄ" * 40)
        
        # STEP 1: Pre-validation
        print("\n" + "=" * 100)
        print("STEP 1: PRE-TRANSLATION VALIDATION")
        print("=" * 100)
        
        self.pre_results = self.validate_codebase("PRE")
        pre_report = self.generate_structure_report(self.pre_results, 'VALIDATION_PRE.txt')
        
        print(f"\n‚úÖ Pre-validation complete!")
        print(f"   üìÑ Report: {pre_report}")
        print(f"   üìä Files: {self.pre_results['stats']['total_files']}")
        print(f"   üîß Functions: {self.pre_results['stats']['total_functions']}")
        print(f"   üèõÔ∏è Classes: {self.pre_results['stats']['total_classes']}")
        print(f"   üá®üá≥ Chinese chars: {self.pre_results['stats']['chinese_characters']:,}")
        
        # STEP 2: Translation
        print("\n" + "=" * 100)
        print("STEP 2: APPLYING TRANSLATIONS")
        print("=" * 100)
        print(f"   üìö Dictionary entries: {len(TRANSLATION_DICT):,}")
        
        self.translation_stats = self.translate_files()
        
        print(f"\n‚úÖ Translation complete!")
        print(f"   üìù Files processed: {self.translation_stats['files_processed']}")
        print(f"   üîÑ Translations applied: {self.translation_stats['translations_applied']:,}")
        print(f"   üá®üá≥ Chinese before: {self.translation_stats['chinese_before']:,}")
        print(f"   üá®üá≥ Chinese after: {self.translation_stats['chinese_after']:,}")
        print(f"   üìâ Reduction: {self.translation_stats['chinese_before'] - self.translation_stats['chinese_after']:,} chars")
        
        # STEP 3: Post-validation
        print("\n" + "=" * 100)
        print("STEP 3: POST-TRANSLATION VALIDATION")
        print("=" * 100)
        
        self.post_results = self.validate_codebase("POST")
        post_report = self.generate_structure_report(self.post_results, 'VALIDATION_POST.txt')
        
        print(f"\n‚úÖ Post-validation complete!")
        print(f"   üìÑ Report: {post_report}")
        print(f"   üìä Files: {self.post_results['stats']['total_files']}")
        print(f"   üîß Functions: {self.post_results['stats']['total_functions']}")
        print(f"   üèõÔ∏è Classes: {self.post_results['stats']['total_classes']}")
        print(f"   üá®üá≥ Chinese chars: {self.post_results['stats']['chinese_characters']:,}")
        
        # STEP 4: Comparison
        print("\n" + "=" * 100)
        print("STEP 4: COMPARISON & RESULTS")
        print("=" * 100)
        
        comparison = {
            'files_delta': self.post_results['stats']['total_files'] - self.pre_results['stats']['total_files'],
            'functions_delta': self.post_results['stats']['total_functions'] - self.pre_results['stats']['total_functions'],
            'classes_delta': self.post_results['stats']['total_classes'] - self.pre_results['stats']['total_classes'],
            'errors_delta': self.post_results['stats']['total_errors'] - self.pre_results['stats']['total_errors'],
            'chinese_reduction': self.pre_results['stats']['chinese_characters'] - self.post_results['stats']['chinese_characters']
        }
        
        print(f"\nüìä DELTA ANALYSIS:")
        print(f"   Files: {comparison['files_delta']:+d}")
        print(f"   Functions: {comparison['functions_delta']:+d}")
        print(f"   Classes: {comparison['classes_delta']:+d}")
        print(f"   Errors: {comparison['errors_delta']:+d}")
        print(f"   Chinese chars removed: {comparison['chinese_reduction']:,}")
        
        # Final verdict
        print("\n" + "üéâ" * 40)
        
        if comparison['errors_delta'] > 0:
            print("‚ö†Ô∏è  WARNING: New errors introduced!")
            print(f"   {comparison['errors_delta']} new error(s) detected")
        elif comparison['errors_delta'] < 0:
            print("‚ú® EXCELLENT: Some errors were fixed!")
        else:
            print("‚úÖ SUCCESS: No new errors introduced!")
        
        if comparison['chinese_reduction'] > 0:
            coverage = (comparison['chinese_reduction'] / self.pre_results['stats']['chinese_characters']) * 100
            print(f"üåê Translation coverage: {coverage:.1f}%")
        else:
            print("‚ö†Ô∏è  No Chinese characters were translated")
        
        print("üéâ" * 40)
        
        # Save comparison report
        comparison_lines = [
            "=" * 100,
            "üìä VALIDATION COMPARISON REPORT",
            "=" * 100,
            "",
            "PRE-TRANSLATION:",
            f"  Files: {self.pre_results['stats']['total_files']}",
            f"  Functions: {self.pre_results['stats']['total_functions']}",
            f"  Classes: {self.pre_results['stats']['total_classes']}",
            f"  Errors: {self.pre_results['stats']['total_errors']}",
            f"  Chinese chars: {self.pre_results['stats']['chinese_characters']:,}",
            "",
            "POST-TRANSLATION:",
            f"  Files: {self.post_results['stats']['total_files']}",
            f"  Functions: {self.post_results['stats']['total_functions']}",
            f"  Classes: {self.post_results['stats']['total_classes']}",
            f"  Errors: {self.post_results['stats']['total_errors']}",
            f"  Chinese chars: {self.post_results['stats']['chinese_characters']:,}",
            "",
            "CHANGES:",
            f"  Files: {comparison['files_delta']:+d}",
            f"  Functions: {comparison['functions_delta']:+d}",
            f"  Classes: {comparison['classes_delta']:+d}",
            f"  Errors: {comparison['errors_delta']:+d}",
            f"  Chinese reduction: {comparison['chinese_reduction']:,}",
            "",
            "VERDICT:",
            "  ‚úÖ No structural damage" if comparison['errors_delta'] <= 0 else "  ‚ö†Ô∏è Errors introduced!",
            "=" * 100
        ]
        
        comparison_report = self.root_path / 'COMPARISON_REPORT.txt'
        with open(comparison_report, 'w', encoding='utf-8') as f:
            f.write('\n'.join(comparison_lines))
        
        print(f"\nüìÑ Full comparison report: {comparison_report}")


def main():
    root = Path('/tmp/Zeeeepa/Phantom')
    validator = ComprehensiveValidator(root)
    validator.run_full_pipeline()


if __name__ == '__main__':
    main()

